import os
import logging
from pathlib import Path
from collections import defaultdict
from langchain.document_loaders import (
    TextLoader,
    UnstructuredFileLoader
)
from langchain.docstore.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain_core.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
import mimetypes
import sys
from dotenv import load_dotenv
sys.path.insert(0, "/root/wang/csst/RepoUnderstanding/code-chunker")
from Chunker import CodeChunker
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
logging.basicConfig(level=logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)


# --------- åŠ è½½æœ¬åœ°æ–‡ä»¶å¹¶è‡ªåŠ¨åˆ¤æ–­ loader ----------
def load_all_local_files(root_dir: str):
    documents = []
    for path in Path(root_dir).rglob("*"):
        if path.is_file():
            try:
                mime_type, _ = mimetypes.guess_type(str(path))
                if mime_type and mime_type.startswith("text"):
                    loader = TextLoader(str(path), encoding="utf-8")
                else:
                    loader = UnstructuredFileLoader(str(path))
                documents.extend(loader.load())
            except Exception as e:
                #print(f"âŒ è·³è¿‡ {path}: {e}")
                pass
    return documents


# --------- æ–‡æœ¬å’Œä»£ç åˆ†å‰²å™¨ ----------
class MultiTypeSplitter:
    def __init__(self, code_chunk_size: int = 1000, text_chunk_size: int = 1000, text_overlap: int = 200):
        self.code_chunk_size = code_chunk_size
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=text_chunk_size,
            chunk_overlap=text_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", "ã€", "ï¼š", " ", "", "\.\s|\!\s|\?\s", "ï¼›|;\s", "ï¼Œ|,\s"]
        )

    def split_documents(self, docs):
        out = []
        for doc in docs:
            path = Path(doc.metadata["source"])
            ext = path.suffix.lower().lstrip(".")
            text = doc.page_content

            try:
                chunks = self._split_code(text, ext, doc.metadata)
                if not chunks:  # å¦‚æœä»£ç åˆ†å—å¤±è´¥æˆ–è¿”å›ç©ºï¼Œå°±å°è¯•æ–‡æœ¬åˆ‡åˆ†
                    chunks = self._split_text(text, doc.metadata)
            except Exception as e:
                #print(f"âŒ æ–‡ä»¶ {path.name} åˆ†å‰²å¤±è´¥ï¼š{e}")
                chunks = self._split_text(text, doc.metadata)

            out.extend(chunks)
        return out

    def _make_doc(self, content, metadata):
        return Document(page_content=content, metadata=metadata)

    def _split_code(self, code_text, ext, metadata):
        chunker = CodeChunker(file_extension=ext, encoding_name="gpt-4")
        raw_chunks = chunker.chunk(code_text, token_limit=self.code_chunk_size)
        return [Document(page_content=code, metadata={**metadata, "chunk_type": "code", "chunk_index": idx})
                for idx, code in raw_chunks.items()]

    def _split_text(self, text, metadata):
        return self.text_splitter.create_documents([text], metadatas=[metadata])


def batch_embed_and_add(chunks, vector_store, embeddings, batch_size=200, max_workers=4):
    """
    åˆ†æ‰¹ä¸º chunks ç”Ÿæˆ embeddings å¹¶å†™å…¥ Chromaï¼ŒåŒæ—¶æ˜¾ç¤ºè¿›åº¦æ¡ã€‚
    - batch_size: æ¯ä¸ªæ‰¹æ¬¡å¤„ç†çš„æ–‡æ¡£å—æ•°
    - max_workers: å¹¶è¡Œçº¿ç¨‹æ•°
    """
    total = len(chunks)
    pbar = tqdm(total=total, desc="ğŸ§® Embedding å’Œ å†™å…¥", unit="doc")
    
    def process_batch(batch):
        # 1) æ‰¹é‡ç”Ÿæˆ embedding
        texts = [doc.page_content for doc in batch]
        embs = embeddings.embed_documents(texts)
        # 2) æ„é€  metadata å’Œ ids
        metadatas = [doc.metadata for doc in batch]
        # 3) å†™å…¥ Chroma
        vector_store.add_embeddings(
            embeddings=embs,
            metadatas=metadatas,
            documents=None  # å¦‚éœ€ä¹Ÿå¯å¡«å…¥åŸæ–‡
        )
        return len(batch)
    
    # ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤šä¸ªæ‰¹æ¬¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for i in range(0, total, batch_size):
            batch = chunks[i:i+batch_size]
            futures.append(executor.submit(process_batch, batch))
        for fut in as_completed(futures):
            processed = fut.result()
            pbar.update(processed)
    pbar.close()

def run_rag(
    repo_path: str,
    questions: list[str],
    embedding_model: str = "text-embedding-3-small",
    llm_model: str = "gpt-4o-2024-08-06",
    chroma_dir: str = "./chroma_db",
    code_chunk_size: int = 1000,
    text_chunk_size: int = 800,
    text_overlap: int = 100,
) -> dict:
    """
    å¯¹æŒ‡å®šæœ¬åœ°ä»“åº“æ‰§è¡Œ RAG æµç¨‹ï¼Œå¹¶è¿”å›é¡¹ç›®æ‘˜è¦åŠé—®ç­”ç»“æœã€‚

    Args:
        repo_path (str): æœ¬åœ°ä»“åº“è·¯å¾„ï¼Œç»å¯¹æˆ–ç›¸å¯¹éƒ½å¯ã€‚
        questions (List[str]): ç”¨æˆ·æå‡ºçš„é—®é¢˜åˆ—è¡¨ã€‚
        embedding_model (str): ç”¨äºç”Ÿæˆ embeddings çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸º text-embedding-3-smallã€‚
        llm_model (str): ç”¨äºæ‘˜è¦å’Œé—®ç­”çš„ LLM æ¨¡å‹ï¼Œé»˜è®¤ä¸º gpt-4o-2024-08-06ã€‚
        chroma_dir (str): Chroma å‘é‡åº“æŒä¹…åŒ–ç›®å½•ã€‚
        code_chunk_size (int): ä»£ç åˆ†å—æ—¶çš„ token é™åˆ¶ã€‚
        text_chunk_size (int): æ–‡æœ¬åˆ†å—æ—¶çš„ chunk å¤§å°ã€‚
        text_overlap (int): æ–‡æœ¬åˆ†å—æ—¶çš„ overlap å¤§å°ã€‚

    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            project_summary (str): é¡¹ç›®çº§æ•´ä½“æ‘˜è¦ã€‚
            qa (List[dict]): é—®ç­”ç»“æœåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
                - question (str)
                - answer (str)
                - sources (List[str])
    """
    # 1. åŠ è½½ & åˆ†å‰²
    docs = load_all_local_files(repo_path)
    splitter = MultiTypeSplitter(
        code_chunk_size=code_chunk_size,
        text_chunk_size=text_chunk_size,
        text_overlap=text_overlap
    )
    chunks = splitter.split_documents(docs)

    # 2. Embedding & å‘é‡åº“
    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        openai_api_base="https://api2.road2all.com/v1",
        model=embedding_model
    )
    vector_store = Chroma(
        collection_name="repo_chunks",
        embedding_function=embeddings,
        persist_directory=chroma_dir
    )
    # åˆ†æ‰¹å†™å…¥
    batch_size = 200
    for i in range(0, len(chunks), batch_size):
        vector_store.add_documents(chunks[i : i + batch_size])

    # 3. é¡¹ç›®çº§æ‘˜è¦
    llm = ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api2.road2all.com/v1",
        model=llm_model,
        temperature=0.2
    )
    # é‡ç”¨ä½ åœ¨æ¨¡å—é¡¶å±‚å®šä¹‰çš„ prompt_template
    chain = load_summarize_chain(
        llm,
        chain_type="map_reduce",
        verbose=False,
        map_prompt=prompt_template,
        combine_prompt=prompt_template
    )
    project_summary = chain.invoke(chunks)["output_text"]

    # 4. é’ˆå¯¹æ¯ä¸ªé—®é¢˜æ‰§è¡Œæ£€ç´¢å¼é—®ç­”
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        return_source_documents=True
    )
    chat_history = []
    qa_results = []
    for q in questions:
        res = qa_chain({"question": q, "chat_history": chat_history})
        qa_results.append({
            "question": q,
            "answer": res["answer"],
            "sources": [doc.metadata["source"] for doc in res["source_documents"]]
        })
        chat_history.append((q, res["answer"]))

    return {
        "project_summary": project_summary,
        "qa": qa_results
    }



# --------- ä¸»æµç¨‹å…¥å£ ----------
def main():
    repo_path = "/root/wang/csst/Galsim"  # cloneåˆ°æœ¬åœ°çš„ä»£ç åº“è·¯å¾„

    print("\nğŸ“‚ æ­£åœ¨åŠ è½½æ–‡ä»¶...")
    documents = load_all_local_files(repo_path)
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡ä»¶\n")

    splitter = MultiTypeSplitter(code_chunk_size=1000, text_chunk_size=800, text_overlap=150)
    chunks = splitter.split_documents(documents)
    print(f"ğŸ§© åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ–‡æ¡£å—\n")

    print("ğŸ”— æ­£åœ¨ç”Ÿæˆ Embedding...")
    load_dotenv()
    embeddings = OpenAIEmbeddings(
        openai_api_key=os.environ["OPENAI_API_KEY"],
        openai_api_base="https://api2.road2all.com/v1",
        model="text-embedding-3-small"
    )

    vector_store = Chroma(collection_name="repo_chunks", embedding_function=embeddings, persist_directory="./chroma_db")
    batch_size = 200
    for i in tqdm(range(0, len(chunks), batch_size), desc="ğŸ§® Embedding+å†™å…¥", unit="chunk"):
        vector_store.add_documents(chunks[i : i + batch_size])

    # Summarization
    logging.getLogger("langchain").setLevel(logging.WARNING)
    llm = ChatOpenAI(
        api_key=os.environ["OPENAI_API_KEY"],
        base_url="https://api2.road2all.com/v1",
        model="gpt-4o-2024-08-06",
        temperature=0.2
    )

    prompt_template = PromptTemplate(
        input_variables=["text"],
        template="""
            ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·ç†è§£ GitHub å¼€æºä»£ç ä»“åº“ã€‚
            ä»¥ä¸‹æ˜¯ä»£ç ä»“åº“çš„æ–‡ä»¶ç»“æ„å’Œéƒ¨åˆ†ä»£ç ç‰‡æ®µï¼š
            {text}

            ä½ çš„ä»»åŠ¡æ˜¯ï¼š
            1. æ€»ç»“ä»£ç ä»“åº“çš„æ•´ä½“æ¦‚è¿°å’ŒåŠŸèƒ½ç›®æ ‡ã€‚
            2. æç‚¼å‡ºä»£ç ç»“æ„ã€ä»£ç ä¸­çš„æ ¸å¿ƒç±»å’Œæ¨¡å—ï¼Œåˆ†æå®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¯´æ˜å®ƒä»¬çš„ä½œç”¨ã€‚
            3. æç‚¼å‡ºå®ç°æ ¸å¿ƒåŠŸèƒ½çš„ä¸»è¦æµç¨‹åŠå…¶æ¶‰åŠçš„ç›¸å…³å†…å®¹ã€‚

            è¯·ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„é¡¹ç›®çº§æ‘˜è¦ã€‚
            """
    )

    summarize_chain = load_summarize_chain(
        llm,
        chain_type="map_reduce",
        verbose=False,
        map_prompt=prompt_template,
        combine_prompt=prompt_template
    )

    print("ğŸ“ æ­£åœ¨ç”Ÿæˆé¡¹ç›®çº§æ‘˜è¦...")
    summary = summarize_chain.invoke(chunks)
    print("\n### ğŸ“˜ é¡¹ç›®çº§æ‘˜è¦ï¼š\n")
    print(summary["output_text"])

    # QA Chat
    print("\nğŸ¤– è¿›å…¥é—®ç­”æ¨¡å¼ï¼ˆè¾“å…¥ exit æˆ– quit é€€å‡ºï¼‰")
    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        return_source_documents=True
    )

    chat_history = []
    while True:
        query = input("\nç”¨æˆ·ï¼š")
        if query.strip().lower() in ("exit", "quit"):
            break
        result = qa({"question": query, "chat_history": chat_history})
        print("\nåŠ©æ‰‹ï¼š", result["answer"])
        chat_history.append((query, result["answer"]))


if __name__ == "__main__":
    main()
